{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7\n",
    "\n",
    "## Aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"gs://reddys-data-for-experimenting/retail-data/all/*.csv\") \\\n",
    "    .coalesce(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "541909"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|   InvoiceDate|UnitPrice|CustomerID|       Country|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|12/1/2010 8:26|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|12/1/2010 8:26|     2.75|     17850|United Kingdom|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|12/1/2010 8:26|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+--------------+---------+----------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|count(StockCode)|\n",
      "+----------------+\n",
      "|          541909|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "df.select(count(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|count(DISTINCT StockCode)|\n",
      "+-------------------------+\n",
      "|                     4070|\n",
      "+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|approx_count_distinct(StockCode)|\n",
      "+--------------------------------+\n",
      "|                            3804|\n",
      "+--------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use this when you an exact number is not needed\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "df.select(approx_count_distinct(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------+----------------------+\n",
      "|first(StockCode, false)|last(StockCode, false)|\n",
      "+-----------------------+----------------------+\n",
      "|                  21544|                85049D|\n",
      "+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import first, last\n",
    "\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|min(Quantity)|max(Quantity)|\n",
      "+-------------+-------------+\n",
      "|       -80995|        80995|\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import min, max\n",
    "\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------------+\n",
      "|sum(Quantity)|sum(DISTINCT Quantity)|\n",
      "+-------------+----------------------+\n",
      "|      5176450|                 29310|\n",
      "+-------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, sumDistinct\n",
    "\n",
    "df.select(sum(\"Quantity\"), sumDistinct(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+----------------+\n",
      "|      manual_avg|    avg_quantity|   mean_quantity|\n",
      "+----------------+----------------+----------------+\n",
      "|9.55224954743324|9.55224954743324|9.55224954743324|\n",
      "+----------------+----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sum, count, avg, expr, mean\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_quantity\"),\n",
    "    avg(\"Quantity\").alias(\"avg_quantity\"),\n",
    "    mean(\"Quantity\").alias(\"mean_quantity\")\n",
    ").selectExpr(\n",
    "    \"total_quantity/total_transactions as manual_avg\",\n",
    "    \"avg_quantity\",\n",
    "    \"mean_quantity\"\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other function can be used in the same way like\n",
    "\n",
    "* skewness\n",
    "* kurtosis\n",
    "* var_pop\n",
    "* stddev_pop\n",
    "* var_samp\n",
    "* stddev_samp\n",
    "* corr\n",
    "* covar_pop\n",
    "* covar_samp\n",
    "* collect_set\n",
    "* collect_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+-----------------+\n",
      "|InvoiceNo|throughFunction|ThroughExpression|\n",
      "+---------+---------------+-----------------+\n",
      "|   536596|              6|                6|\n",
      "|   536938|             14|               14|\n",
      "|   537252|              1|                1|\n",
      "|   537691|             20|               20|\n",
      "|   538041|              1|                1|\n",
      "+---------+---------------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, expr\n",
    "\n",
    "df.groupBy(\"InvoiceNo\") \\\n",
    "    .agg(count(\"StockCode\").alias(\"throughFunction\"),\n",
    "         expr(\"count(StockCode)\").alias(\"ThroughExpression\")\n",
    "        ).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rollups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- InvoiceNo: string (nullable = true)\n",
      " |-- StockCode: string (nullable = true)\n",
      " |-- Description: string (nullable = true)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- InvoiceDate: string (nullable = true)\n",
      " |-- UnitPrice: double (nullable = true)\n",
      " |-- CustomerID: integer (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+--------------+--------------+\n",
      "|    InvoiceDate|       Country|total_quantity|\n",
      "+---------------+--------------+--------------+\n",
      "|           null|          null|       5176450|\n",
      "|1/10/2011 10:04|United Kingdom|           -29|\n",
      "|1/10/2011 10:04|          null|           -29|\n",
      "|1/10/2011 10:07|          EIRE|            -4|\n",
      "|1/10/2011 10:07|          null|            -4|\n",
      "|1/10/2011 10:08|          null|           -14|\n",
      "|1/10/2011 10:08|United Kingdom|           -14|\n",
      "|1/10/2011 10:32|United Kingdom|           260|\n",
      "|1/10/2011 10:32|          null|           260|\n",
      "|1/10/2011 10:35|          null|           408|\n",
      "+---------------+--------------+--------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF = df.rollup(\"InvoiceDate\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    "  .selectExpr(\"InvoiceDate\", \"Country\", \"`sum(Quantity)` as total_quantity\")\\\n",
    "  .orderBy(\"InvoiceDate\")\n",
    "rolledUpDF.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+-------------+\n",
      "|InvoiceDate|             Country|sum(Quantity)|\n",
      "+-----------+--------------------+-------------+\n",
      "|       null|               Japan|        25218|\n",
      "|       null|           Australia|        83653|\n",
      "|       null|            Portugal|        16180|\n",
      "|       null|             Germany|       117448|\n",
      "|       null|                null|      5176450|\n",
      "|       null|United Arab Emirates|          982|\n",
      "|       null|                 RSA|          352|\n",
      "|       null|             Finland|        10666|\n",
      "|       null|             Lebanon|          386|\n",
      "|       null|         Unspecified|         3300|\n",
      "|       null|              Cyprus|         6317|\n",
      "|       null|      Czech Republic|          592|\n",
      "|       null|              Norway|        19247|\n",
      "|       null|     Channel Islands|         9479|\n",
      "|       null|           Hong Kong|         4769|\n",
      "|       null|                 USA|         1034|\n",
      "|       null|           Singapore|         5234|\n",
      "|       null|               Spain|        26824|\n",
      "|       null|  European Community|          497|\n",
      "|       null|             Denmark|         8188|\n",
      "+-----------+--------------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df.cube(\"InvoiceDate\", \"Country\").agg(sum(col(\"Quantity\")))\\\n",
    "  .select(\"InvoiceDate\", \"Country\", \"sum(Quantity)\").orderBy(\"InvoiceDate\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}