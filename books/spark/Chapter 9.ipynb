{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 9\n",
    "\n",
    "## Reading and Writing data in Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading\n",
    "\n",
    "```python\n",
    "spark.read.format(\"csv\")\n",
    "  .option(\"mode\", \"FAILFAST\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"path\", \"path/to/file(s)\")\n",
    "  .schema(someSchema)\n",
    "  .load()\n",
    "```\n",
    "\n",
    "### Writing\n",
    "\n",
    "```python\n",
    "dataframe.write.format(\"csv\")\n",
    "  .option(\"mode\", \"OVERWRITE\")\n",
    "  .option(\"dateFormat\", \"yyyy-MM-dd\")\n",
    "  .option(\"path\", \"path/to/file(s)\")\n",
    "  .save()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.postgresql:postgresql:42.1.1 pyspark-shell'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for reading and writing `CSVs`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from Google Cloud Storage\n",
    "csvFile = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"mode\", \"FAILFAST\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"gs://reddys-data-for-experimenting/flight-data/csv/2010-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "csvFile.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing data back to Google Cloud Storage\n",
    "csvFile.write.format(\"csv\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .save(\"gs://reddys-data-for-experimenting/output/chapter9/tsv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for reading and writing `JSON`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading data from Google Cloud Storage\n",
    "jsonData = spark.read.format(\"json\").option(\"mode\", \"FAILFAST\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"gs://reddys-data-for-experimenting/flight-data/json/2010-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jsonData.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonData.write.format(\"json\").mode(\"overwrite\").save(\"gs://reddys-data-for-experimenting/output/chapter9/json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for reading and writing `Parquet`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetData = spark.read.format(\"parquet\")\\\n",
    "  .load(\"gs://reddys-data-for-experimenting/flight-data/parquet/2010-summary.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetData.write.format(\"json\").mode(\"overwrite\").save(\"gs://reddys-data-for-experimenting/output/chapter9/parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for reading and writing `ORC`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcData = spark.read.format(\"orc\").load(\"gs://reddys-data-for-experimenting/flight-data/orc/2010-summary.orc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcData.write.format(\"json\").mode(\"overwrite\").save(\"gs://reddys-data-for-experimenting/output/chapter9/orc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example for Reading and writing with `JDBC`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code snippet\n",
    "\n",
    "``` python\n",
    "driver = \"org.sqlite.JDBC\"\n",
    "path = \"gs://reddys-data-for-experimenting//flight-data/jdbc/my-sqlite.db\"\n",
    "url = \"jdbc:sqlite:\" + path\n",
    "tablename = \"flight_info\"\n",
    "\n",
    "dbDataFrame = spark.read.format(\"jdbc\").option(\"url\", url)\\\n",
    "  .option(\"dbtable\", tablename).option(\"driver\",  driver).load()\n",
    "    \n",
    "pgDF = spark.read.format(\"jdbc\")\\\n",
    "  .option(\"driver\", \"org.postgresql.Driver\")\\\n",
    "  .option(\"url\", \"jdbc:postgresql://database_server\")\\\n",
    "  .option(\"dbtable\", \"schema.tablename\")\\\n",
    "  .option(\"user\", \"username\").option(\"password\", \"my-secret-password\").load()\n",
    "```\n",
    "\n",
    "Also Spark does query push down, so that it fetches as little data as possible from the underlying datasource.\n",
    "\n",
    "You can also write to SQL Databases usinf spark\n",
    "\n",
    "```python\n",
    "csvFile.write.jdbc(newPath, tablename, mode=\"overwrite\", properties=props)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing data into partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.repartition(5).write.format(\"csv\") \\\n",
    "    .save(\"gs://reddys-data-for-experimenting/output/chapter9/partitioned-csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile.write.mode(\"overwrite\").partitionBy(\"DEST_COUNTRY_NAME\")\\\n",
    "  .save(\"gs://reddys-data-for-experimenting/output/chapter9/partitioned-by-key-parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing data into buckets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bukceting is only supported in `Scala` and not in `Python` at the moment\n",
    "\n",
    "```scala\n",
    "val numberBuckets = 10\n",
    "val columnToBucketBy = \"count\"\n",
    "\n",
    "csvFile.write.format(\"parquet\")\n",
    "  .mode(\"overwrite\")\n",
    "  .bucketBy(numberBuckets, columnToBucketBy)\n",
    "  .save(\"gs://reddys-data-for-experimenting/output/chapter9/partitioned-csv\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}