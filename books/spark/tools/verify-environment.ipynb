{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: Make sure your environment is configured properly\n",
    "\n",
    "This notebook is just intended to check if you are able to access spark and use it though a jupyter notebook. So, you dont need to pay any attention to the code in it. Just go ahead and run all the cells and check to see if you are able to instanciate spark and use it.\n",
    "\n",
    "* I am using Jupyter Notebooks powered with Cloud Dataproc as I can provision and use the cluster from anywhere\n",
    "* The persistance of the data is not attached to the compute and is stored in GCS in this case\n",
    "* Make sure you don't store any state in any of the cloud compute instances as it not the best practice to do so. Instead make use of one of the following\n",
    "    - `AWS S3` \n",
    "    - `GCP GCS`\n",
    "    - `Azure OS`\n",
    "    \n",
    "**Provsisioning a Cloud Dataproc cluster**\n",
    "\n",
    "You run the following command to do so\n",
    "\n",
    "```sh\n",
    "# Cluster provisioning command\n",
    "# Make sure you have already logged in using 'gcloud auth login'\n",
    "\n",
    "./gcp/dataproc \\\n",
    "          --gcloud-email=sample@gmail.com \\\n",
    "          --project-id=spark-dataproc-cluster \\\n",
    "          --cluster-name=test-spark-cluster \\\n",
    "          --bucket=dataproc-statestore \\\n",
    "          --action=create\n",
    "```\n",
    "\n",
    "Follow the instructions in the `README.md` for more information. All code can e found in my [Codebooks Repository on github](https://github.com/reddy-s/codebooks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure spark context\n",
    "\n",
    "* App name\n",
    "* Spark Master location (Spark Master config is needed only if running in a cluster mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'spark.eventLog.enabled', u'true'),\n",
       " (u'spark.dynamicAllocation.minExecutors', u'1'),\n",
       " (u'spark.executor.memory', u'2688m'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_HOSTS',\n",
       "  u'test-spark-cluster-m'),\n",
       " (u'spark.ui.proxyBase', u'/proxy/application_1569758767276_0003'),\n",
       " (u'spark.driver.port', u'38757'),\n",
       " (u'spark.yarn.am.memory', u'640m'),\n",
       " (u'spark.history.fs.logDirectory',\n",
       "  u'hdfs://test-spark-cluster-m/user/spark/eventlog'),\n",
       " (u'spark.eventLog.dir', u'hdfs://test-spark-cluster-m/user/spark/eventlog'),\n",
       " (u'spark.executor.instances', u'2'),\n",
       " (u'spark.serializer.objectStreamReset', u'100'),\n",
       " (u'spark.executorEnv.PYTHONPATH',\n",
       "  u'/usr/lib/spark/python/lib/py4j-0.10.7-src.zip:/usr/lib/spark/python/:<CPS>{{PWD}}/pyspark.zip<CPS>{{PWD}}/py4j-0.10.7-src.zip'),\n",
       " (u'spark.submit.deployMode', u'client'),\n",
       " (u'spark.ui.filters',\n",
       "  u'org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter'),\n",
       " (u'spark.org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter.param.PROXY_URI_BASES',\n",
       "  u'http://test-spark-cluster-m:8088/proxy/application_1569758767276_0004'),\n",
       " (u'spark.shuffle.service.enabled', u'true'),\n",
       " (u'spark.driver.memory', u'1920m'),\n",
       " (u'spark.scheduler.mode', u'FAIR'),\n",
       " (u'spark.hadoop.hive.execution.engine', u'mr'),\n",
       " (u'spark.driver.maxResultSize', u'960m'),\n",
       " (u'spark.yarn.jars', u'local:/usr/lib/spark/jars/*'),\n",
       " (u'spark.scheduler.minRegisteredResourcesRatio', u'0.0'),\n",
       " (u'spark.executor.id', u'driver'),\n",
       " (u'spark.app.name', u'PySparkShell'),\n",
       " (u'spark.dynamicAllocation.maxExecutors', u'10000'),\n",
       " (u'spark.yarn.historyServer.address', u'test-spark-cluster-m:18080'),\n",
       " (u'spark.executor.extraJavaOptions',\n",
       "  u'-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance'),\n",
       " (u'spark.master', u'yarn'),\n",
       " (u'spark.rpc.message.maxSize', u'512'),\n",
       " (u'spark.rdd.compress', u'True'),\n",
       " (u'spark.app.id', u'application_1569758767276_0004'),\n",
       " (u'spark.driver.appUIAddress',\n",
       "  u'http://test-spark-cluster-m.europe-west2-a.c.spark-dataproc-cluster.internal:4040'),\n",
       " (u'spark.executorEnv.OPENBLAS_NUM_THREADS', u'1'),\n",
       " (u'spark.yarn.isPython', u'true'),\n",
       " (u'spark.executor.cores', u'1'),\n",
       " (u'spark.sql.parquet.cacheMetadata', u'false'),\n",
       " (u'spark.driver.host',\n",
       "  u'test-spark-cluster-m.europe-west2-a.c.spark-dataproc-cluster.internal'),\n",
       " (u'spark.dynamicAllocation.enabled', u'true'),\n",
       " (u'spark.ui.showConsoleProgress', u'true'),\n",
       " (u'spark.sql.cbo.enabled', u'true'),\n",
       " (u'spark.driver.extraJavaOptions',\n",
       "  u'-Dflogger.backend_factory=com.google.cloud.hadoop.repackaged.gcs.com.google.common.flogger.backend.log4j.Log4jBackendFactory#getInstance')]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.getConf().getAll()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://test-spark-cluster-m.europe-west2-a.c.spark-dataproc-cluster.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ff39eb4bed0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a datafrom from a range of numbers\n",
    "df = spark.range(10).toDF(\"numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numbers: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      0|\n",
      "|      1|\n",
      "|      2|\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "|      6|\n",
      "|      7|\n",
      "|      8|\n",
      "|      9|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(numbers=0), Row(numbers=1)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying tuples\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the number of entries in the DF\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access to Data\n",
    "\n",
    "All the code in this repository will read data from a `GCS` bucket which I own. I have set the billing property to this repo as `requester pays`. If you want, you mauy use it at your expense.\n",
    "\n",
    "Make sure data in available in one of the GCS buckets to validate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gs://reddys-data-for-experimenting/flight-data/csv/2010-summary.csv\n",
      "gs://reddys-data-for-experimenting/flight-data/csv/2011-summary.csv\n",
      "gs://reddys-data-for-experimenting/flight-data/csv/2012-summary.csv\n",
      "gs://reddys-data-for-experimenting/flight-data/csv/2013-summary.csv\n",
      "gs://reddys-data-for-experimenting/flight-data/csv/2014-summary.csv\n",
      "gs://reddys-data-for-experimenting/flight-data/csv/2015-summary.csv\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil ls gs://reddys-data-for-experimenting/flight-data/csv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "flightData = spark \\\n",
    "    .read \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"gs://reddys-data-for-experimenting/flight-data/csv/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Ireland', count=344)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#92 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#92 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#90,ORIGIN_COUNTRY_NAME#91,count#92] Batched: false, Format: CSV, Location: InMemoryFileIndex[gs://reddys-data-for-experimenting/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightData.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#92 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#92 ASC NULLS FIRST, 5)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#90,ORIGIN_COUNTRY_NAME#91,count#92] Batched: false, Format: CSV, Location: InMemoryFileIndex[gs://reddys-data-for-experimenting/flight-data/csv/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightData.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME=u'United States', ORIGIN_COUNTRY_NAME=u'Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME=u'Moldova', ORIGIN_COUNTRY_NAME=u'United States', count=1)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightData.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping spark context\n",
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Info: If all of the above cells have executed sucessfully, please go ahead and start of with the book as mentioned in the readme.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}