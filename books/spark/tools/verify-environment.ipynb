{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark: Make sure your environment is configured properly\n",
    "\n",
    "This notebook is just intended to check if you are able to access spark and use it though a jupyter notebook. So, you dont need to pay any attention to the code in it. Just go ahead and run all the cells and check to see if you are able to instanciate spark and use it.\n",
    "\n",
    "* I am using Azure Notebooks for these notebooks, as I can access it from anywhere.\n",
    "* Note that the persistance of the notebook instance is only 60 days from the time it goes inactive. \n",
    "* Make sure you don't store any state here. Make use of \n",
    "    - `AWS S3` \n",
    "    - `GCP GCS`\n",
    "    - `Azure OS`\n",
    "    \n",
    "**Run in Cluster-Mode**\n",
    "\n",
    "You can run all of these tutorials in a cluster mode by running the following command\n",
    "\n",
    "```sh\n",
    "docker-compose -f spark-cluster.yml up -d\n",
    "```\n",
    "\n",
    "And then log into `Jupyter Notebook` by going to `http://localhost:8888` and the spark master runs on `http://localhost:8080`. All the notebooks and other resources are mounted for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import pyspark\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure spark context\n",
    "\n",
    "* App name\n",
    "* Spark Master location (Spark Master config is needed only if running in a cluster mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = pyspark.SparkConf().setAppName(\"Spark Test\").setMaster(os.environ['SPARK_MASTER']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f253cf0ec9a3:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Spark Test</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f22413d7c18>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize spark context\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "spark = pyspark.sql.SparkSession.builder \\\n",
    "                    .config(conf=conf) \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "sc\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a datafrom from a range of numbers\n",
    "df = spark.range(10).toDF(\"numbers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- numbers: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Printing the schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|numbers|\n",
      "+-------+\n",
      "|      0|\n",
      "|      1|\n",
      "|      2|\n",
      "|      3|\n",
      "|      4|\n",
      "|      5|\n",
      "|      6|\n",
      "|      7|\n",
      "|      8|\n",
      "|      9|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Displaying the dataframe\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(numbers=0), Row(numbers=1)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying tuples\n",
    "df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counting the number of entries in the DF\n",
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping spark context\n",
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Info: If all of the above cells have executed sucessfully, please go ahead and start of with the book as mentioned in the readme.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
